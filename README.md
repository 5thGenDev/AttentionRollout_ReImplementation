# AttentionRollout ReImplementation
Adapt original reimplementation of Attention Rollout by https://github.com/jacobgil/vit-explain.

**Motivation: ***Isolate the impact of each following topic*** on how the model pay attentions, its accuracy and GPUtime (Flop and GFlop):**

- Representation vs Based Learning to pretrain Vision Transformer (ViT):
     - [ ] Based Learning:
     - [ ] Self-supervised Vision Transformer 2021: https://github.com/Sara-Ahmed/SiT
           
- Latest optimizers:
     - [ ] SAM: https://github.com/davda54/sam 

- Different Attention-layer in ViT:
     - [ ] Hydra Attention: Hydra Attention: Efficient Attention with Many Heads: https://arxiv.org/abs/2209.07484 
     - [ ] Accurate Image Restoration with Attention Retractable Transformer: https://github.com/gladzhang/ART

- U-Net backbone for Diffusion Model:
     - [ ] Art style diffusion: https://github.com/ChenDarYen/ArtFusion
     - [ ] Prompt-to-Prompt Image Editing with Cross-Attention Control: https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/2725. *Learning how to use to general stable-diffusion is a tough learning curve **but worth it in long term**

- ViT backbone for Diffusion Model:
     - [ ] Fast Training of Diffusion Models with Masked Transformers: https://github.com/Anima-Lab/MaskDiT
     - [ ] **VDT: An Empirical Study for Video Diffusion with Transformers**: https://github.com/RERV/VDT
     - [ ] Masked Diffusion Transformer is a Strong Image Synthesizer: https://github.com/sail-sg/MDT
     - [ ] Paper to read as baseline: Exploring Transformer Backbones for Image Diffusion Models


## Tools to be used to visualize Attention Map ##
- [ ] Attention Rollout
- [ ] Gradient-based Attention Rollout
- [ ] ????
